---
title: "CMM014 - Cálculo Numérico" 
subtitle: "Solução do problema de quadrados mínimos discretos usando SVD"
author: ["João Pedro de Lima", "Raymundo Eduardo Pilz"]
# date: "Dezembro 2024"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: styles.css
    footer: 'Estatística e Ciência de Dados - UFPR'
    incremental: true
    transition: slide
    output-file: slides.html
---

```{r setup, include=FALSE}
# Carregando bibliotecas
library(tidyverse)
library(rsm)
library(ggplot2)
library(scales)
library(plotly)
```


## O que é a técnica SVD e por que usar?

* SVD = Decomposição em Valores Singulares
* Toda matriz $A (m×n)$ pode ser escrita como:

  $$
  A = U \Sigma V^T
  $$
* $U, V$: ortogonais | $\Sigma$: diagonal com valores singulares

## Propriedades da SVD

* $U$ e $V$ são ortogonais
* Valores singulares $\Sigma \leq 0$
* Sempre existe, mesmo para matrizes não quadradas
* Ordenados de forma decrescente

## Que tipo de problemas a SVD resolve?

* Sistemas sobredeterminados ($m > n$)
* Ajustes de curvas (regressão linear/múltipla)
* Compressão de imagens e dados
* Redução de dimensionalidade (PCA)


## Fundamentação Teórica

- O método dos mínimos quadrados busca ajustar uma função$f(x, \beta)$ a um conjunto de dados observados$(x_i, y_i)$.
- Define-se o **resíduo** como:
 $$
  r_i = y_i - f(x_i, \beta)
 $$
- O objetivo é minimizar a soma dos quadrados residuais:
 $$
  S = \sum_{i=1}^{n} r_i^2
 $$
- Isso leva à estimativa dos parâmetros$\beta$ mais adequados ao modelo.


## Modelo Linear

- Um dos modelos mais simples é a reta:
 $$
  f(x, \beta) = \beta_0 + \beta_1 x
 $$
- Pode ser generalizado para múltiplas variáveis e escrito em forma matricial:
 $$
  A x = b
 $$
- O sistema é geralmente **sobredeterminado**, ou seja, mais equações que incógnitas (m > n).


## Abordagens Clássicas

- **Solução Analítica**: $x = (A^T A)^{-1} A^T b$
  - Rápida, mas instável se$A^T A$ for mal-condicionada
- **Decomposição QR**: $A = QR$
  - Estável, mas sensível à colinearidade
- **Decomposição SVD**: $A = U \Sigma V^T$
  - Alta estabilidade numérica, mesmo com colinearidade
  
## SVD aplicada a mínimos quadrados 


* Problema clássico:

  $$
  \min \|Ax - b\|_2
  $$
* SVD fornece:

  $$
  x = A^+ b = V \Sigma^+ U^T b
  $$

## Exemplo 1 – Regressão Linear: Consumo × Renda {.smaller .scrollable transition="slide"}


|$i$ | Consumo ($y$) | Renda ($x$) |
|:---:|--------------:|------------:|
| 1   | 122           | 139         |
| 2   | 114           | 126         |
| 3   | 86            | 90          |
| 4   | 134           | 144         |
| 5   | 146           | 163         |
| 6   | 107           | 136         |
| 7   | 68            | 61          |
| 8   | 117           | 62          |
| 9   | 71            | 41          |
| 10  | 98            | 120         |

Dados (Wikipedia)

## Exemplo 1 – Regressão Linear: Consumo × Renda

- Modelo linear:
 $$
  f(x) = \beta_0 + \beta_1 x
 $$

- Resultado via SVD:
  -$\beta_0 = 52{,}69$
  -$\beta_1 = 0{,}4954$

- Interpretação:
  > A cada aumento de 1 unidade de Renda, espera-se um aumento de 0,4954 unidades no Consumo.


## Previsão com o modelo ajustado

- Fórmula estimada:
 $$
  \text{Consumo} = 52{,}69 + 0{,}4954 \times \text{Renda}
 $$

- Exemplo:
  - Renda = 100 → Consumo previsto ≈ 102,23
  - Renda = 2025 → Consumo previsto ≈ 1061,22

- A SVD permitiu obter coeficientes estáveis, mesmo com possível colinearidade nos dados.


## Exemplo 2 – Modelo Exponencial

- Dados: população do país de 2000 a 2020

- Modelo:
 $$
  P(t) = a \cdot e^{bt}
 $$

- Após logaritmo:
 $$
  \ln P(t) = \ln a + bt
 $$

- Transformado em modelo linear → aplicado SVD

- Estimativas obtidas:
  -$a \approx 1000$,$b \approx 0{,}041$


## Carregamento dos dados {.smaller .scrollable transition="slide"}

::: panel-tabset

### Código

~~~julia

# Dados de crescimento populacional
anos = [2000, 2005, 2010, 2015, 2020]
pop = [1000, 1200, 1500, 1900, 2500]

# Normaliza o tempo e aplica log nos dados
t = anos .- minimum(anos)
y = log.(pop)

# Monta o sistema Ax = y
A_exp = hcat(ones(length(t)), t)

# Decomposição SVD
U, S, Vt = svd(A_exp)

# Cálculo da pseudo-inversa
tolerance_exp = max(size(A_exp)...) * eps(Float64) * S[1]
Sigma_plus_exp_diag = [s > tolerance_exp ? 1.0/s : 0.0 for s in S]
A_plus_exp = Vt * Diagonal(Sigma_plus_exp_diag) * U'
x_coeffs = A_plus_exp * y_log
c0, c1 = x_coeffs

# Recupera parâmetros a e b
a_exp = exp(c0)
b_exp = c1
~~~

### Gráfico

![](img/fig1.svg){.absolute top="80" center="0" width="80%" height="600"}

:::


## Pontos fortes e fracos


**Pontos fortes:**

* Estável numericamente
* Aplica-se a qualquer matriz
* Solução de norma mínima

**Pontos fracos:**

* Mais custosa computacionalmente
* Pode ser “overkill” para problemas simples

## Aplicações ideais



* Engenharia e estatística
* Visão computacional (compressão de imagens)
* Análise de dados (PCA)
* Machine Learning (redução de dimensionalidade)

## Conclusão {auto-animate="true"}


* SVD oferece solução robusta para mínimos quadrados
* Permite explorar problemas de modelagem com confiança
* Aplicações reais e práticas
* Desafios: entender a decomposição, transformar problemas


## {}

### Obrigado!






