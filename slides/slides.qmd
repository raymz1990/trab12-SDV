---
title: "CMM014 - Cálculo Numérico" 
subtitle: "Solução do problema de quadrados mínimos discretos usando SVD"
author: ["João Pedro de Lima", "Raymundo Eduardo Pilz"]
# date: "Dezembro 2024"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: styles.css
    footer: 'Estatística e Ciência de Dados - UFPR'
    incremental: true
    transition: slide
    output-file: slides.html
---

```{r setup, include=FALSE}
# Carregando bibliotecas
library(tidyverse)
library(rsm)
library(ggplot2)
library(scales)
library(plotly)
```

## Introdução

- Ajustar curvas a dados reais é essencial em modelagem.
- O método dos mínimos quadrados busca minimizar a soma dos quadrados dos resíduos.
- SVD oferece uma solução estável, mesmo em casos com colinearidade ou matrizes mal-condicionadas.


## Problema de Mínimos Quadrados

-  $Ax = b$ nem sempre tem solução exata.

-   O problema de mínimos quadrados consiste em determinar um vetor$x$ que minimiza a expressão$\|b - Ax\|_2$.

-   A denominação desse problema se deve ao fato de estarmos minimizando a soma dos quadrados dos resíduos.

## Teorema

Sejam$A \in \mathbb{R}^{n \times m}$,$b \in \mathbb{R}^n$ ($n < m$) e posto($A$)$= r$. O problema de mínimos quadrados associado ao sistema linear$Ax = b$ possui exatamente uma solução de norma mínima.

## Fundamentação Teórica

- O método dos mínimos quadrados busca ajustar uma função$f(x, \beta)$ a um conjunto de dados observados$(x_i, y_i)$.
- Define-se o **resíduo** como:
 $$
  r_i = y_i - f(x_i, \beta)
 $$
- O objetivo é minimizar a soma dos quadrados residuais:
 $$
  S = \sum_{i=1}^{n} r_i^2
 $$
- Isso leva à estimativa dos parâmetros$\beta$ mais adequados ao modelo.


## Modelo Linear

- Um dos modelos mais simples é a reta:
 $$
  f(x, \beta) = \beta_0 + \beta_1 x
 $$
- Pode ser generalizado para múltiplas variáveis e escrito em forma matricial:
 $$
  A x = b
 $$
- O sistema é geralmente **sobredeterminado**, ou seja, mais equações que incógnitas (m > n).


## Abordagens Clássicas

- **Solução Analítica**:$x = (A^T A)^{-1} A^T b$
  - Rápida, mas instável se$A^T A$ for mal-condicionada
- **Decomposição QR**:$A = QR$
  - Estável, mas sensível à colinearidade
- **Decomposição SVD**:$A = U \Sigma V^T$
  - Alta estabilidade numérica, mesmo com colinearidade


## Por que SVD?

- Evita a inversão de matrizes quase singulares
- Lida bem com:
  - Colunas quase linearmente dependentes
  - Matrizes retangulares
  - Problemas mal-condicionados
- Permite identificar e **filtrar valores singulares pequenos**
- Retorna a solução de **norma mínima**:
 $$
  x = V \Sigma^+ U^T b
 $$
  
## Metodologia

- A ideia central é resolver o problema de mínimos quadrados com base na decomposição:
 $$
  A = U \Sigma V^T
 $$
- Utilizamos a pseudo-inversa de$A$:
 $$
  A^+ = V \Sigma^+ U^T
 $$
- A solução de norma mínima é então:
 $$
  x = A^+ b
 $$

## Exemplo 1 – Regressão Linear: Consumo × Renda {.smaller .scrollable transition="slide"}


|$i$ | Consumo ($y$) | Renda ($x$) |
|:---:|--------------:|------------:|
| 1   | 122           | 139         |
| 2   | 114           | 126         |
| 3   | 86            | 90          |
| 4   | 134           | 144         |
| 5   | 146           | 163         |
| 6   | 107           | 136         |
| 7   | 68            | 61          |
| 8   | 117           | 62          |
| 9   | 71            | 41          |
| 10  | 98            | 120         |

Dados (Wikipedia)

## Exemplo 1 – Regressão Linear: Consumo × Renda

- Modelo linear:
 $$
  f(x) = \beta_0 + \beta_1 x
 $$

- Resultado via SVD:
  -$\beta_0 = 52{,}69$
  -$\beta_1 = 0{,}4954$

- Interpretação:
  > A cada aumento de 1 unidade de Renda, espera-se um aumento de 0,4954 unidades no Consumo.


## Previsão com o modelo ajustado

- Fórmula estimada:
 $$
  \text{Consumo} = 52{,}69 + 0{,}4954 \times \text{Renda}
 $$

- Exemplo:
  - Renda = 100 → Consumo previsto ≈ 102,23
  - Renda = 2025 → Consumo previsto ≈ 1061,22

- A SVD permitiu obter coeficientes estáveis, mesmo com possível colinearidade nos dados.


## Exemplo 2 – Modelo Exponencial

- Dados: população do país de 2000 a 2020

- Modelo:
 $$
  P(t) = a \cdot e^{bt}
 $$

- Após logaritmo:
 $$
  \ln P(t) = \ln a + bt
 $$

- Transformado em modelo linear → aplicado SVD

- Estimativas obtidas:
  -$a \approx 1000$,$b \approx 0{,}041$


## Comparação: Dados reais vs. Modelo

- A curva estimada se ajusta bem ao crescimento exponencial dos dados reais.

- A SVD possibilitou:
  - Tratamento numérico estável
  - Ajuste de norma mínima
  - Resultado interpretável


## Carregamento dos dados {.smaller .scrollable transition="slide"}

::: panel-tabset

### Código

~~~julia

# Dados de crescimento populacional
anos = [2000, 2005, 2010, 2015, 2020]
pop = [1000, 1200, 1500, 1900, 2500]

# Normaliza o tempo e aplica log nos dados
t = anos .- minimum(anos)
y = log.(pop)

# Monta o sistema Ax = y
A_exp = hcat(ones(length(t)), t)

# Decomposição SVD
U, S, Vt = svd(A_exp)

# Cálculo da pseudo-inversa
tolerance_exp = max(size(A_exp)...) * eps(Float64) * S[1]
Sigma_plus_exp_diag = [s > tolerance_exp ? 1.0/s : 0.0 for s in S]
A_plus_exp = Vt * Diagonal(Sigma_plus_exp_diag) * U'
x_coeffs = A_plus_exp * y_log
c0, c1 = x_coeffs

# Recupera parâmetros a e b
a_exp = exp(c0)
b_exp = c1
~~~

### Gráfico

![](img/fig1.svg){.absolute top="80" center="0" width="80%" height="600"}

:::

## Conclusão {auto-animate="true"}

- A SVD se mostrou eficiente para resolver problemas de mínimos quadrados:

  - Evita instabilidades numéricas  
  - Aplica-se a matrizes não quadradas ou mal-condicionadas  
  - Permite estimar soluções de norma mínima  
  - Adequada para modelos lineares e transformações lineares



## Referências Bibliográficas

- STRANG, Gilbert. *Álgebra Linear e suas Aplicações*. Cengage, 2010.  
- Wikipedia – Método dos Mínimos Quadrados  
- Wikipedia – Decomposição em Valores Singulares  
- TREFETHEN, L. N. & BAU III, D. *Numerical Linear Algebra*. SIAM, 1997.



## {}

### Obrigado!






